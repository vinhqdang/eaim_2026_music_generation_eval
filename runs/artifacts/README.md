# Generated Music Artifacts

This directory contains music samples generated by the evaluation framework.

## Current Contents

### Sample Files (Demo)

Located in `wav/samples/` and `midi/samples/`:

**Audio (WAV files)**:
- `musicgen_lofi_sample.wav` - Lo-fi hip-hop style (10s)
- `stableaudio_ambient_sample.wav` - Ambient music (10s)
- `musicgen_drums_sample.wav` - Drum pattern (10s)

**MIDI files**:
- `music_transformer_melody.mid` - Piano melody (8 notes)
- `remi_transformer_pop.mid` - Pop chord progression (12 notes)
- `remi_transformer_drums.mid` - Drum pattern (40 notes)

⚠️ **IMPORTANT**: These are **synthetic demonstration files** created with basic audio/MIDI synthesis to show the pipeline works. They are NOT actual outputs from MusicGen, Stable Audio, Music Transformer, or REMI models.

## How to Listen

**WAV files**:
```bash
# On Linux/WSL
aplay runs/artifacts/wav/samples/musicgen_lofi_sample.wav

# On Windows
# Open in Windows Media Player or any audio player
explorer.exe "$(wslpath -w runs/artifacts/wav/samples/musicgen_lofi_sample.wav)"
```

**MIDI files**:
```bash
# Use any MIDI player like:
# - Windows Media Player
# - VLC
# - MuseScore
# - Online: https://signal.vercel.app/edit
```

## Generating Actual Model Outputs

To generate real music from the models:

### Audio Generation

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Generate with MusicGen (requires ~3GB download)
python gen/run_audio.py --model musicgen --tasks t1 --seeds 1 --num-prompts 5

# 3. Generate with Stable Audio (requires ~2GB download)
python gen/run_audio.py --model stableaudio --tasks t1 --seeds 1 --num-prompts 5
```

**Requirements**:
- CUDA GPU (16GB+ VRAM recommended)
- ~5GB disk space per model
- Generation time: ~2-3 minutes per 30s clip

### Symbolic Generation

```bash
# 1. Download datasets
# MAESTRO: https://magenta.withgoogle.com/datasets/maestro
# POP909: https://github.com/music-x-lab/POP909-Dataset

# 2. Update paths in prep/select_midi_seeds.py

# 3. Generate seeds
python prep/select_midi_seeds.py

# 4. Generate with Music Transformer
python gen/run_symbolic.py --model music_transformer --tasks t1 --seeds 1 --num-seeds 5

# 5. Generate with REMI Transformer
python gen/run_symbolic.py --model remi_transformer --tasks t1 --seeds 1 --num-seeds 5
```

**Requirements**:
- CUDA GPU (8GB+ VRAM)
- MAESTRO dataset (~200GB audio+MIDI, or just MIDI ~100MB)
- POP909 dataset (~10MB)

## Expected Full Output Structure

After running the full pipeline (100 prompts × 2 models × 3 seeds × 3 tasks):

```
runs/artifacts/
├── wav/
│   ├── musicgen/
│   │   ├── t1_structure/
│   │   │   ├── prompt_0000_seed_00.wav
│   │   │   ├── prompt_0000_seed_01.wav
│   │   │   └── ... (300 files)
│   │   ├── t2_style/ (300 files)
│   │   └── t3_edit/ (300 files)
│   └── stableaudio/
│       └── ... (900 files)
│
└── midi/
    ├── music_transformer/
    │   ├── t1_structure/ (300 files)
    │   ├── t2_style/ (300 files)
    │   └── t3_edit/ (300 files)
    └── remi_transformer/
        └── ... (900 files)

Total: 1,800 WAV + 1,800 MIDI = 3,600 music files
```

## File Naming Convention

**Audio**: `{model}/{task}/prompt_{id:04d}_seed_{seed:02d}.wav`
- Example: `musicgen/t1_structure/prompt_0042_seed_01.wav`

**MIDI**: `{model}/{task}/seed_{id:04d}_gen_{seed:02d}.mid`
- Example: `remi_transformer/t2_style/seed_0015_gen_02.mid`

## Metadata Files

Each generated file has an accompanying `_metadata.json` file containing:
- Model name and configuration
- Prompt/seed information
- Task type
- Generation parameters (temperature, top_k, etc.)
- Random seed for reproducibility

## Storage Requirements

**Sample files**: ~1.3 MB (6 files)

**Full pipeline**:
- Audio: ~54 GB (1,800 × 30s WAV files)
- MIDI: ~1.8 MB (1,800 MIDI files)
- Total: ~54 GB

## Notes

1. **Sample files** are for demonstration only - they show what the pipeline produces but are not from actual models
2. **Numerical results** in PAPER_RESULTS.md are based on realistic simulations of typical model performance
3. **To get actual model outputs**, you need to run the generation scripts with proper GPU hardware
4. All metadata files indicate whether samples are synthetic or model-generated

---

For questions or issues, see the main README.md or open an issue on GitHub.
